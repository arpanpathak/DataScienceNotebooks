{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGoDVPr5nAk0iTOyaFpRfR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpanpathak/DataScienceNotebooks/blob/main/BERT_For_Product_Categorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš€ Fine-Tuning SLMs for Enterprise Product Categorization"
      ],
      "metadata": {
        "id": "5ihrtv1e2h-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## The Business Case: Why Small Language Models (SLMs)?\n",
        "\n",
        "> In real-world production systems (e.g., e-commerce search, inventory tagging), Latency and Cost are the primary constraints. While LLMs (Llama-3 70B, GPT-4) are \"smart,\" they are inefficient for high-throughput classification.\n",
        "\n",
        "| Metric | Large Language Model (LLM) | Small Language Model (DistilBERT) |\n",
        "| :--- | :--- | :--- |\n",
        "| Parameters | 7B - 175B | 66M |\n",
        "| Inference Time | 500ms - 2s (GPU) | ~15ms (CPU) |\n",
        "| Cost | High (Requires A100s) | Low (Runs on T4 or CPU) |\n",
        "| Task Suitability | Creative Generation, Reasoning | Pattern Recognition, Classification |\n",
        "\n",
        "Conclusion: For a defined taxonomy (e.g., Amazon > Hardware > Laptops), a fine-tuned SLM often achieves >95% accuracy with 1/100th the compute cost of an LLM.\n",
        "\n",
        "## Mathematical Intuition: The Transformer Architecture\n",
        "\n",
        "> DistilBERT is a distilled version of BERT. To understand it, we must look at the mathematical operations inside a Transformer encoder.\n",
        "\n",
        "### A. Input Embedding & Positional Encoding\n",
        "\n",
        "Transformers have no inherent sense of order (unlike RNNs). We must inject position information mathematically.\n",
        "\n",
        "Given a token sequence $X = [x_1, \\dots, x_n]$, we map them to learned embeddings $E \\in \\mathbb{R}^{d_{model}}$. We add a fixed positional vector $P$:\n",
        "\n",
        "$$Input(pos) = E(x_{pos}) + P(pos)$$\n",
        "\n",
        "Where the positional encoding $P$ is defined by sine/cosine frequencies:\n",
        "\n",
        "$$P_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "\n",
        "$$P_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "\n",
        "### B. Scaled Dot-Product Attention\n",
        "\n",
        "This is the \"brain\" of the model. It calculates how much focus (attention) token $i$ should give to token $j$.\n",
        "\n",
        "For a specific head, we project the input $X$ into Query ($Q$), Key ($K$), and Value ($V$) matrices:\n",
        "\n",
        "\n",
        "$$Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V$$\n",
        "\n",
        "The attention score is calculated as:\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "$QK^T$: The dot product measures semantic similarity between tokens.\n",
        "\n",
        "$\\sqrt{d_k}$: Scaling factor to stabilize gradients (prevents softmax saturation).\n",
        "\n",
        "$V$: The actual information content we want to aggregate based on the attention scores.\n",
        "\n",
        "## Why DistilBERT? (Knowledge Distillation)\n",
        "\n",
        "DistilBERT is trained to mimic a larger \"Teacher\" model (BERT-Base). The training objective minimizes a triple loss function to transfer knowledge:\n",
        "\n",
        "$$\\mathcal{L} = \\alpha \\mathcal{L}_{ce} + \\beta \\mathcal{L}_{cos} + \\gamma \\mathcal{L}_{mlm}$$\n",
        "\n",
        "$\\mathcal{L}_{ce}$ (Distillation Loss): KL-Divergence between the Teacher's soft probabilities ($t_i$) and the Student's soft probabilities ($s_i$). The temperature $T$ controls the \"softness\" of the probability distribution to capture rich relationships between classes.\n",
        "\n",
        "\n",
        "$$\\mathcal{L}_{ce} = \\sum_i t_i \\log \\left( \\frac{t_i}{s_i} \\right)$$\n",
        "\n",
        "$\\mathcal{L}_{cos}$ (Cosine Embedding Loss): Aligns the hidden state vectors directions between Teacher and Student.\n",
        "\n",
        "$\\mathcal{L}_{mlm}$ (Masked Language Modeling): Standard BERT training (predicting masked words).\n",
        "\n",
        "## The Fine-Tuning Algorithm\n",
        "\n",
        "We adapt the pre-trained DistilBERT for our specific Product Category Classification task.\n",
        "\n",
        "### Step 1: Contextual Representation\n",
        "\n",
        "We pass our product description string $x$ through the DistilBERT model. We extract the hidden state of the special classification token [CLS] from the last layer.\n",
        "\n",
        "$$h_{[CLS]} = \\text{DistilBERT}(x) \\in \\mathbb{R}^{768}$$\n",
        "\n",
        "### Step 2: The Classification Head\n",
        "\n",
        "We project this 768-dimension vector down to our $K$ product categories using a learned weight matrix $W \\in \\mathbb{R}^{K \\times 768}$ and bias $b$.\n",
        "\n",
        "$$z = W h_{[CLS]} + b$$\n",
        "\n",
        "### Step 3: Probability Distribution (Softmax)\n",
        "\n",
        "We convert the logits $z$ into probabilities $\\hat{y}$ for each class $k$:\n",
        "\n",
        "$$\\hat{y}_k = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
        "\n",
        "### Step 4: Optimization Objective (Cross-Entropy Loss)\n",
        "\n",
        "We minimize the difference between our predicted distribution $\\hat{y}$ and the true one-hot label $y$ (where $y_c=1$ for the correct category).\n",
        "\n",
        "$$\\mathcal{L}(\\theta) = -\\sum_{c=1}^{K} y_c \\log(\\hat{y}_c)$$\n",
        "\n",
        "### Step 5: Weight Update (AdamW)\n",
        "\n",
        "We update all model parameters $\\theta$ (both DistilBERT weights and Classification Head weights) using backpropagation.\n",
        "\n",
        "$$\\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$"
      ],
      "metadata": {
        "id": "wUO2WShp73-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install torch transformers tqdm"
      ],
      "metadata": {
        "id": "jPK6Sy5a2-jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Models and Datasets (BYOD - Bring Your Own Dataset)\n",
        "\n",
        "## Sample Dataset\n",
        "| Description | Category |\n",
        "| :--- | :--- |\n",
        "| Dell XPS 13 Laptop 16GB RAM 512GB SSD | Electronics > Computers > Laptops |\n",
        "| Nike Air Max Running Shoes Size 10 | Clothing > Men > Shoes |\n",
        "| Sony WH-1000XM4 Noise Canceling Headphones | Electronics > Audio > Headphones |\n",
        "| Apple MacBook Pro M1 Chip 13-inch | Electronics > Computers > Laptops |\n",
        "| Adidas Ultraboost 21 Sneakers | Clothing > Men > Shoes |\n",
        "| Bose QuietComfort 45 Bluetooth Headphones | Electronics > Audio > Headphones |\n",
        "| Lenovo ThinkPad X1 Carbon Gen 9 | Electronics > Computers > Laptops |\n",
        "| Puma T-Shirt Cotton Black | Clothing > Men > Tops |\n",
        "| JBL Flip 5 Waterproof Portable Bluetooth Speaker | Electronics > Audio > Speakers |\n",
        "| Harry Potter and the Sorcerer's Stone Hardcover | Books > Fiction > Fantasy |\n",
        "| The Great Gatsby Paperback | Books > Fiction > Classics |\n",
        "| Python Crash Course 2nd Edition | Books > Non-Fiction > Education |"
      ],
      "metadata": {
        "id": "k8c_yHQw2tOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "class ProductDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for Product Classification.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        descriptions: list[str],\n",
        "        categories: list[int],\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        max_len: int = 128\n",
        "    ):\n",
        "        self.descriptions = descriptions\n",
        "        self.categories = categories\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.descriptions)\n",
        "\n",
        "    def __getitem__(self, item_idx):\n",
        "        text = str(self.descriptions[item_idx])\n",
        "        label = self.categories[item_idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'description_text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"\n",
        "    Handles loading CSV, encoding labels, and splitting data.\n",
        "    \"\"\"\n",
        "    def __init__(self, csv_path: str):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "        # Encode string categories to integers\n",
        "        self.df['label_encoded'] = self.label_encoder.fit_transform(self.df['category'])\n",
        "\n",
        "        self.num_classes = len(self.label_encoder.classes_)\n",
        "        self.id2label = {i: label for i, label in enumerate(self.label_encoder.classes_)}\n",
        "        self.label2id = {label: i for i, label in enumerate(self.label_encoder.classes_)}\n",
        "\n",
        "    def get_data(self):\n",
        "        return self.df['description'].values, self.df['label_encoded'].values\n",
        "\n",
        "    def decode_label(self, label_id: int) -> str:\n",
        "        return self.label_encoder.inverse_transform([label_id])[0]"
      ],
      "metadata": {
        "id": "Uxdq2HKB2u22"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Engine"
      ],
      "metadata": {
        "id": "0hxl4gxG3W8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
        "    \"\"\"\n",
        "    Training loop for one epoch.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_examples = 0\n",
        "\n",
        "    # TQDM progress bar for live monitoring\n",
        "    progress_bar = tqdm(data_loader, desc=\"  Training\", leave=False)\n",
        "\n",
        "    for data in progress_bar:\n",
        "        input_ids = data['input_ids'].to(device)\n",
        "        attention_mask = data['attention_mask'].to(device)\n",
        "        labels = data['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        _, preds = torch.max(logits, dim=1)\n",
        "        correct_predictions += torch.sum(preds == labels)\n",
        "        total_examples += labels.size(0)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        final_loss += loss.item()\n",
        "\n",
        "        # Update progress bar with current loss\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = final_loss / len(data_loader)\n",
        "    accuracy = correct_predictions.double() / total_examples\n",
        "\n",
        "    return accuracy.item(), avg_loss\n",
        "\n",
        "def eval_fn(data_loader, model, device):\n",
        "    \"\"\"\n",
        "    Evaluation loop for validation.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_examples = 0\n",
        "\n",
        "    progress_bar = tqdm(data_loader, desc=\"  Validating\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in progress_bar:\n",
        "            input_ids = data['input_ids'].to(device)\n",
        "            attention_mask = data['attention_mask'].to(device)\n",
        "            labels = data['labels'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "            total_examples += labels.size(0)\n",
        "\n",
        "            final_loss += loss.item()\n",
        "\n",
        "    avg_loss = final_loss / len(data_loader)\n",
        "    accuracy = correct_predictions.double() / total_examples\n",
        "\n",
        "    return accuracy.item(), avg_loss"
      ],
      "metadata": {
        "id": "xKBMFbjC300k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Script"
      ],
      "metadata": {
        "id": "hn7Nv9tf38K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# --- Configuration ---\n",
        "# Note that dataset is not included. Make your own dataset and categories.\n",
        "CSV_FILE = 'data/sample_products.csv'\n",
        "MAX_LEN = 64  # Short length for sample; increase to 128+ for real data\n",
        "BATCH_SIZE = 4 # Small batch size for sample; increase to 16/32\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 2e-5\n",
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "\n",
        "def main():\n",
        "    # 1. Device Setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # 2. Data Preparation\n",
        "    print(\"Loading and processing data...\")\n",
        "    processor = DataProcessor(CSV_FILE)\n",
        "    descriptions, labels = processor.get_data()\n",
        "\n",
        "    # Split data\n",
        "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "        descriptions, labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"Training samples: {len(train_texts)}\")\n",
        "    print(f\"Validation samples: {len(val_texts)}\")\n",
        "    print(f\"Number of classes: {processor.num_classes}\")\n",
        "    print(f\"Classes: {processor.id2label}\")\n",
        "\n",
        "    # 3. Tokenizer & Datasets\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    train_dataset = ProductDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
        "    val_dataset = ProductDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # 4. Model Initialization\n",
        "    # We pass id2label and label2id so the model config saves the mapping for inference later\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        num_labels=processor.num_classes,\n",
        "        id2label=processor.id2label,\n",
        "        label2id=processor.label2id\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    # 5. Optimizer & Scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # 6. Training Loop\n",
        "    best_accuracy = 0\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        train_acc, train_loss = train_fn(train_loader, model, optimizer, device, scheduler)\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "        val_acc, val_loss = eval_fn(val_loader, model, device)\n",
        "        print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_accuracy:\n",
        "            best_accuracy = val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.bin')\n",
        "            print(\"-> Best model saved!\")\n",
        "\n",
        "    print(\"\\nTraining complete.\")\n",
        "\n",
        "    # Optional: Quick Inference Check\n",
        "    print(\"\\n--- Inference Check ---\")\n",
        "    sample_text = \"Lenovo laptop with 16GB RAM\"\n",
        "    inputs = tokenizer(sample_text, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "    print(f\"Input: {sample_text}\")\n",
        "    print(f\"Predicted: {processor.decode_label(predicted_class_id)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYCL3RA34Adg",
        "outputId": "f578c465-c6e4-4b1f-dd2d-b4e4aa09b11b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading and processing data...\n",
            "Training samples: 9\n",
            "Validation samples: 3\n",
            "Number of classes: 8\n",
            "Classes: {0: 'Books > Fiction > Classics', 1: 'Books > Fiction > Fantasy', 2: 'Books > Non-Fiction > Education', 3: 'Clothing > Men > Shoes', 4: 'Clothing > Men > Tops', 5: 'Electronics > Audio > Headphones', 6: 'Electronics > Audio > Speakers', 7: 'Electronics > Computers > Laptops'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 1/5\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.1073 | Train Acc: 0.1111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss:   2.0769 | Val Acc:   0.0000\n",
            "\n",
            "Epoch 2/5\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.0886 | Train Acc: 0.2222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss:   2.0739 | Val Acc:   0.3333\n",
            "-> Best model saved!\n",
            "\n",
            "Epoch 3/5\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.0078 | Train Acc: 0.4444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss:   2.0777 | Val Acc:   0.3333\n",
            "\n",
            "Epoch 4/5\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.9374 | Train Acc: 0.4444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss:   2.0800 | Val Acc:   0.3333\n",
            "\n",
            "Epoch 5/5\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.0035 | Train Acc: 0.6667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss:   2.0809 | Val Acc:   0.3333\n",
            "\n",
            "Training complete.\n",
            "\n",
            "--- Inference Check ---\n",
            "Input: Lenovo laptop with 16GB RAM\n",
            "Predicted: Electronics > Computers > Laptops\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ]
}